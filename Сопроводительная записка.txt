Целью задания стаяла разработка программы, которая по заданной ссылке собирала полезную информацию с сайта и сохраняла её в текстовом файле на персональном. В условия были требования для форматирования: 
•	Длинна строки не должна превышать 80 символов;
•	Реализовать переносы слов, в случае превышения требуемой длинны строки;
•	Ссылки должны быть заключены в квадратные скобки;
•	Между абзацами должна быть пустая строка.
В качестве усложнения задания предлагалось:
•	Имя выходного файла должно соответствовать имени статьи из ссылки на сайт,  а директория сохранения файла должна повторять структуру древа сайта;
•	Программа должна поддаваться настройке- с заданием шаблонов из отдельного файла.
В результате разработки были выполнены все требования, кроме возможности настройки. Среда реализации MS Windows 10.
Для реализации были использованы модули:
import sys – для работы с командной строкой 
import os – для создания директории папок
import urllib.parse – для работы с url
import urllib.request – для загрузки файла
from html.parser import * - для парсинга страницы
import textwrap – для переноса слов
В программ5е были созданы следующие классы и методы
class myHtmlParser(HTMLParser): - потомок класса HTMLParser
def __init__(self, **kwargs): - задание свойств класса;
def translete(self): - сборка отдельных параграфов и частей документа в готовый к сохранению файл;
def handle_starttag(self, tag, attrs): - контроль открытия тега
def handle_data(self, data: str): - обработка содержания между тегами
def handle_endtag(self, tag: str): - контроль закрытия тега
class CreateFiles(): - создание и сохранение файла с полезной информацией 
def __init__(self, **kwargs): - задание свойств класса;
def extractHTML(self, url1): - получение данных с HTML страницы
Алгоритм решения
1)	Получаем из командной строки ссылку на страницу;
2)	Создаём экземпляр класса CreateFiles, который принимает весь исходный код страницы, содержащий в себе html теги и полезную информацию
3)	Создаём экземпляр класса myHtmlParser, который используя переписанные методы обработки тегов, получает полезную информацию в требуемом виде.
4)	Передаём эту информацию обратно экземпляру класса CreateFiles, который создает и сохраняет текстовый файл в директории. 
Текст программы
import sys
import os
import urllib.parse
import urllib.request
from html.parser import *
import textwrap


class myHtmlParser(HTMLParser): #класс для обработки исходного кода html страницы

    def __init__(self, **kwargs):
         super().__init__()
         self.maxlen=80 # максимальная длинна строки
         self.textc=[] #список параграфов
         self.recording = False #флаг записи информации
         self.href = 0    #атирбут флага ссылки
         self.fl = 0      #флаг ссылки
         self.links = ""  #ссылка

    def translete(self): # преобразование HTML страницы в текстовый блок
        textin=[] #обнуление переменных
        wrtext=''
        textout=''
        for parag in self.textc:
            textin.append(''.join(parag)) # получение списка параграфов
        for parag in textin:
            if parag[-1]!='f': #контроль фалга параграфа с ссылкой
                wrtext+=parag # добавляем к текущему параграфу параграф с ссылкой
                textout += "\n".join(textwrap.wrap(wrtext, self.maxlen)) + "\n\n" # формируем конечный файл
                wrtext=''
            else:
                wrtext=parag[:-1] # убираем флаг параграфа с сыылкой и запоминаем во временную переменную
        return textout # возвращаем сформированый файл

    def handle_starttag(self, tag, attrs): # проверка и определение открытого тегоа
         if tag == 'title': # проверка на заголовок
             self.recording = True
         if tag == 'p': # проверка на текст
             self.recording = True
         if tag == 'a': # проверка на ссылку
             if self.recording == True :
                 self.href=1
                 # находим аттрибут адреса ссылки
                 for attr in attrs:
                     if attr[0] == 'href': # проверка что атрибут содержит ссылку
                         self.links = attr[1] #получание адреса ссылки
         if (tag != 'p'and tag != 'title' and self.recording == True):
             self.fl = 1 # флаг ссылки

    def handle_endtag(self, tag): # проверка и определение зактрытого тега
        if tag == 'title':# проверка на заголовок
            self.recording = False
        if tag == 'p': # проверка на текст
            self.recording = False
        if tag == 'a': # проверка на ссылку
            if self.recording == True :
        #     if self.recording == False :
                self.href=0
        if (tag != 'p' and tag != 'title' and self.recording == True):
            self.fl = 0 # флаг ссылки

    def handle_data(self, data):# обработка информации между тегами
        if self.recording: # проверка что можно заносить параграф в список
            if self.href==1:  # является ли элемент ссылкой
                self.textc.append("["+self.links+"] "+data+'f')# внесение ссылки в словарь параграфов
            else :
                if self.fl!= 1: # если не является ссылкой
                    self.textc.append(data) #внесение текста в словарь параграфов

class CreateFiles(): # класс для создания и сохранения файла с полезной информацией

    def __init__(self, **kwargs):
         self.__url1=''  # ссылка на страницу сайта

    def extractHTML(self, url1): # получение данных с HTML страницы
        resource = urllib.request.urlopen(url1)#получение исходного кода страницы
        content =  resource.read().decode(resource.headers.get_content_charset()) #кодировка странцы для понимания её питоном

        SaveText=myHtmlParser() # создание экземплюяра класса
        SaveText.feed(content) # передача исходного кода
        SaveText.close()
        textout_f=SaveText.translete() # вызов метода обработки информации

        result_parse = urllib.parse.urlparse(url1) # получаем адрес ссылки и домен сайта
        hostname = result_parse.hostname #получаем домен сайта
        path = result_parse.path#получаем url который идет после домена
        if path[-1] == "/":  # убираем слеши первый и последний
            path = path[1:-1]
        path_items = path.split('/') # делаем разбивку на список
        source_name = path_items[-1] # получаем имя статьи
        file_name = source_name+'.txt'# дабавляем расширение файла
        # Формирование пути сохранения файла.
        relative_path = hostname + "/" + "/".join(path_items[0:-1]) #  формируем из списка директорию для сохранения
        if not os.path.exists(relative_path): #  проверка существует ли задаваемая директория
            os.makedirs(relative_path) #  создаем директорию
        # Запись в файл.
        f = open(relative_path + "/" + file_name, 'w', encoding='utf-8') # создаем файл в с уканием директории
        f.write(textout_f) #  вносим преобразованный текст
        f.close() # закрываем файл

if len(sys.argv)>1: # проверка на ввод ссылки на страницу
     url1=sys.argv[1] # передаем в переменную адрес сайта
else:
     url1="Ссылка не введена"
html1=CreateFiles()
html1.extractHTML(url1)

Ссылки на сайты для проверки работы программы:
https://lenta.ru/news/2019/02/03/mummies/  - выгрузка прошла успешно. 
https://www.gazeta.ru/politics/news/2019/02/19/n_12657829.shtml?updated – файл создан успешно. В текст помимо полезной информации попали контакты с подвала страницы, так как они также были помечены тегом <p>.
https://vz.ru/news/2019/2/18/964876.html - выгрузка прошла успешно.
https://iz.ru/847096/2019-02-18/zhirinovskii-prizval-vygnat-lazareva-s-evrovideniia-iz-za-ego-slov-o-kryme - файл создан успешно. В текст помимо полезной информации попали контакты с подвала страницы, так как они также были помечены тегом <p>.
https://rg.ru/2019/02/18/vrach-rasskazal-kak-na-zdorove-rossiian-povliiaet-superlunie.html - выгрузка прошла успешно.
https://lenta.ru/news/2019/02/19/t_34/ - выгрузка прошла успешно.
https://www.mk.ru/science/2019/02/18/blizitsya-moshhneyshaya-magnitnaya-burya-fevralya-kak-pozabotitsya-o-sebe.html - файл создан успешно. В текст помимо полезной информации попали контакты с подвала страницы, так как они также были помечены тегом <p>.
https://news.rambler.ru/starlife/41743784-yakubovich-ne-vyderzhal-gneta-posle-skandala-s-edoy/ - программа не смогла обработать данный сайт, т.к. он содержит иные теги разметки.

Варианты улучшения программы:
1)	Создать визуальный и функциональный интерфейс с возможностями настройки программы (шрифт текста и заголовка; отступы; межстрочный интервал; цвет шрифта и фона; формат сохранения; изменение директории сохранения);
2)	Возможность сохранения картинки/фото из статьи с последующей загрузкой в текстовый файл.
3)	Отправка файла на электронную почту или загрузка на собственный сайт (дайджест новостей)
4)	Составить полный список желаемых новостных сайтов и добиться 100% корректной загрузки полезной информации.
5)	Создать окно превью- которое бы отображало все последние новости, что бы пользователю не приходилось открывать браузер, а видеть напрямую из программы  заголовки последних новостей с возможностью скачать выбранную статью.

